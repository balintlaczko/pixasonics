{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixasonics: An Image Sonification Toolbox for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixasonics is a library for interactive audiovisual image analysis and exploration, through image sonification. That is, it is using real-time audio and visualization to listen to image data: to map between image features and acoustic parameters. This can be handy when you need to work with a large number of images, image stacks, or hyper-spectral images (involving many color channels) where visualization becomes limiting, challenging, and potentially overwhelming.\n",
    "\n",
    "With pixasonics, you can launch a little web application (running in a Jupyter notebook), where you can load images, probe their data with various feature extraction methods, and map the extracted features to parameters of synths, devices that make sound. You can do all this in real-time, using a visual interface, you can remote-control the interface programmatically, record sound real-time, or non-real-time, with a custom script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you are in a hurry..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ;pip install pixasonics\n",
    "\n",
    "# quick workflow with a simple example\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import MeanChannelValue\n",
    "from pixasonics.synths import Theremin\n",
    "\n",
    "# create a new app\n",
    "app = App() # by default 500x500 pixels\n",
    "\n",
    "# load an image from file\n",
    "app.load_image_file(\"images/test.jpg\")\n",
    "\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app.attach_feature(mean_red) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin(name=\"MySine\")\n",
    "# attach the Theremin to the app\n",
    "app.attach_synth(theremin) # this adds it to the pipeline, so that its audio output is patched into the global audio graph\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach_mapper(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toolbox Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixasonics (at the moment) is expected to run in a Jupyter notebook environment. (Nothing stops you from using it in the terminal, but it is not optimized for that yet.)\n",
    "\n",
    "At the center of pixasonics is the `App` class. This represents a template pipeline where all your image data, feature extractors, synths and mappers will live. The App also comes with a graphical user interface (UI). At the moment it is expected that you only create one `App` at a time, which will control the global real-time audio server. (And every time you create an `App` it will reset the audio graph.)\n",
    "\n",
    "When you have your app, you load an image (either from a file, or from a numpy array) which will be displayed in the `App` canvas. Note that _currently_ your image data height and width dimensions (the first two) will be downsampled to the `App`'s `image_size` creation argument, which is a tuple of `(500, 500)` pixels by default.\n",
    "\n",
    "Then you can explore the image data with a Probe (represented by the yellow rectangle on the canvas) using your mouse or trackpad. The Probe is your \"stethoscope\" on the image, and more technically, it is the sub-matrix of the Probe that is passed to all `Feature` objects in the pipeline.\n",
    "\n",
    "Speaking of which, you can extract visual features using the `Feature` base class, or any of its convenience abstractions (e.g. `MeanChannelValue`). Currently only basic statistical reductions are supported, such as `\"mean\"`, `\"median\"`, `\"min\"`, `\"max\"`, `\"sum\"`, `\"std\"` (standard deviation) and `\"var\"` (variance). `Feature` objects also come with a UI that shows their current values and global/running min and max. There can be any number of different `Feature`s attached to the app, and all of them will get the same Probe matrix as input.\n",
    "\n",
    "Image features are to be mapped to synthesis parameters, that is, to the settings of sound-making gadgets. (This technique is called \"Parameter Mapping Sonification\" in the literature.) All synths (and audio) in pixasonics are based on the fantastic [signalflow library](https://signalflow.dev/). For now, there are 5 synth classes that you can use (and many more are on the way): `Theremin`, `Oscillator`, `FilteredNoise`, and `SimpleFM`. Each synth comes with a UI, where you can tweak the parameters (or see them being modulated by `Mapper`s) in real-time.\n",
    "\n",
    "What connects the output of a `Feature` and the input parameter of a Synth is a `Mapper` object. There can be multiple `Mapper`s reading from the same `Feature` buffer and a Synth can have multiple `Mapper`s modulating its different parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `App` class is at the core of the pixasonics workflow. The `App` is where you load your image data, where you move the probe, the `App` controls the real-time audio server, and it represents the pipeline of `Feature`s connected to Synths with `Mapper`s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To showcase the `App` and its functionality, let's create a basic scene, using an image from the [CELLULAR open dataset](https://zenodo.org/records/8315423) and map the mean red channel value to the frequency of a Theremin (a simple sine wave generator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import MeanChannelValue\n",
    "from pixasonics.synths import Theremin\n",
    "\n",
    "# create a new app\n",
    "app = App() # by default 500x500 pixels\n",
    "\n",
    "# load an image from file\n",
    "app.load_image_file(\"images/cellular_dataset/merged_8bit/Timepoint_001_220518-ST_C03_s1.jpg\")\n",
    "\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app.attach_feature(mean_red) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin()\n",
    "# attach the Theremin to the app\n",
    "app.attach_synth(theremin) # this adds it to the pipeline, so that its audio output is patched into the global audio graph\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach_mapper(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things just happened. The app created (or restarted) the global audio graph and the UI popped up: a canvas with the image on the left, and various settings panes on the right. The size of the canvas is determined by the `image_size` argument at the creation of the `App` object and cannot be changed afterwards. It is a tuple corresponding to `(height, width)`, and it is `(500, 500)` by default. All images that you load into the app will be __resized__ to this height and width! (This might be changed/improved in the future.) This means, that, at least currently, the app does not respect the aspect ratio of your input image, it will stretch or shrink it to whatever `image_size` your `App` has. Luckily the image we loaded is 2048x2048 with a square aspect ratio, so the default `image_size` was fine.\n",
    "\n",
    "Now let's look to the right where the various settings panes live. These can be opened and closed, one at a time. Here is a brief summary of what you can find in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click on \"Audio Settings\" on the top right to open this pane. Here you can control the _global_ audio settings of the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Audio switch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an audio switch at the top left, and a volume slider next to it. To have any sound produced by the app, you need to turn on audio, either on the UI, or by evaluating:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.audio = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still no sound from the app, huh? This is because we haven't interacted with the canvas yet. Try clicking on a few of the cells. Then try to click and drag to hear a continuous change in frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Volume Slider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next to the audio switch there is the Master Volume slider where you can set the loudness of the app's sound output in decibels (dB). You can either control it via the UI or the `master_volume` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_volume = -24 # check the slider after running this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-time recording to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the audio switch and the master slider, there is a pair of widgets that let you record the real-time output of the app. Leave the file name at the default `\"recording.wav\"`, hit the Record button, click and drag around the image, then click on the record button again to stop recording.\n",
    "\n",
    "Let's listen to what you've done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "display(Audio(\"recording.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The global recording is meant to be a quick-and-easy way to record your results. If you want to be more precise, (or faster than real-time) there are methods to render your results in non-real-time, which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Master Envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the recording section you find the Master Envelope, which controls how fast the sound fades in and out when you interact with the canvas. It is a traditional Attack-Decay-Sustain-Release (ADSR) curve that is applied to the volume of the global audio output. Here is how it works in a nutshell:\n",
    "- Attack: the time for the sound to fade in (in seconds),\n",
    "- Decay: the time to fade to the Sustain level (in seconds),\n",
    "- Sustain: the level (amplitude) to sustain indefinitely, until we release the mouse button (or deactivate the Probe),\n",
    "- Release the time for the sound to fade out after we release the mouse button (or deactivate the Probe).\n",
    "\n",
    "There are some fields to set these parameters, and a little drawing that visualizes the proportions of the different time segments. On the bottom, there is also a \"Duration\" value that shows the total duration of the envelope (that is Attack time + Decay time + Release time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set a bit slower envelope to illustrate how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.5\n",
    "app.master_envelope.release = 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set a more percussive envelope:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.01\n",
    "app.master_envelope.decay = 0.01\n",
    "app.master_envelope.sustain = 0.1\n",
    "app.master_envelope.release = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's reset it to a more neutral default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.master_envelope.attack = 0.1\n",
    "app.master_envelope.decay = 0.01\n",
    "app.master_envelope.sustain = 1\n",
    "app.master_envelope.release = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the future Envelopes will have more functionality in pixasonics, but for now there is only a global Master Envelope. Now let's close the Audio Settings pane and move on to Display Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, we have started to stray a bit far from the cell where our app UI lives... Luckily, we can get a synced copy down here if we evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aaah, much better. Let's open the Display Settings. As the name suggests, this is where you can find settings related to how the image is displayed in the app canvas. These will __only affect the display__ though, the underlying image data (that may be HDR, or have many color channels or image layers) will not be affected by these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the top of the pane you can find two checkboxes: \"Normalize display\" and \"Global normalization\". More often than not we need to normalize images to understand (or even, simply, to see!) the image content better. Traditionally, the normalization is performed individually on all color channels (Red, Green, and Blue in \"normal\" images). Keeping it channel-wise can help remove imbalances between the channels, like the green tint here. Check \"Normalize display\" in the UI, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably noticed that the background became much less green-tinted. Let's try also checking \"Global normalization\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...aaand now the gren tint is back. It makes sense, because when normalization is set to global, the algorithm will scale all pixels according to the minimum and maximum pixel value apparent in __any__ of the channels. Since in the original image the green tint was the result of generally much higher green pixel values, applying global normalization brought the tint back. Let's turn it off for now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.normalize_display_global = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel Offset and Layer Offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, under the checkboxes we have two (currently disabled) sliders, labelled \"Channel Offset\" and \"Layer Offset\". We will use these when we read numpy arrays instead of single image files (stay tuned!). For now, let's move on to the Probe Settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probe Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the Display Settings pane, and open the Probe Settings. This is where you set everything related to the Probe (remember, our image \"stethoscope\") and how you interact with it. The Probe is represented by a rectangle drawn over the image. When the Probe is active, the rectangle is red, and when the Probe is inactive, the rectangle is yellow. (Also, when the Probe is active, the `App` will evaluate all attached mappings in the pipeline and unmute audio for all attached synths.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Probe Width and Height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The width and height of the Probe can vary from 1x1 pixel up to the app's `image_size`. Try moving the sliders and observe how the shape of the yellow rectangle on the canvas changes. There is not really a \"right\" setting for this, it is highly dependent on the content of your image, the area of pixels you want to send to feature extraction, and generally, the kind of sonification you want to create. At an extreme, you can create horizontal or vertical scan lines for the whole image like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal scanning (a vertical scan line)\n",
    "app.probe_width = 1\n",
    "app.probe_height = app.image_size[0] # remember that image size is given as (height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical scanning (a horizontal scan line)\n",
    "app.probe_width = app.image_size[1]\n",
    "app.probe_height = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, you probably want to set the Probe dimensions to something smaller to fit the content of the image. Let's set it to fit that nice, bright cell in the middle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_width = 25\n",
    "app.probe_height = 25\n",
    "app.probe_x = 233\n",
    "app.probe_y = 252"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, you guessed it: we can programmatically move the probe around using the `probe_x` and `probe_y` properties of our `App`. Setting the dimensions and the position of the probe like this will be the basis of scripting custom paths and render them non-real-time (more on that later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction modes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now something fun: interaction. So far we have used the \"Hold\" mode, that is, sound will be activated (and all existing mappers evaluated in the pipeline) while the mouse button is held down. This mode is comfortable when you want to quickly inspect (and listen to) the various parts of your image.\n",
    "\n",
    "But sometimes you may want to activate the Probe, and then leave it on while you experiment with, let's say, changing synthesis parameters. Or you just want to free your mouse to use it somewhere else without stopping the sound output. This is what \"Toggle\" mode is for. You can change it on the UI by clicking on the \"Toggle\" button, or programmatically like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.interaction_mode = \"toggle\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can double-click on the probe to activate it. Double-click again to deactivate it. It could be sometimes comfortable to activate the Probe, and just click on various parts in the image (here, on different cells) to get an abrupt comparison of their sounding mappings. \n",
    "\n",
    "Try to activate the toggle and then click on the different cells to find out which one produces the highest pitch! Since we mapped the mean red channel value to the theremin frequency, and since in this image red fluorescent protein indicates the level of autophagy going on in the cell, we can intuitively find the cell with the most active autophagy by simple clicking around and listening."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, there is a checkbox for having the Probe __always__ follow your mouse, even when you don't hold the mouse button. Beyond this being simply comfortable for your hand if you are making sound from the image continuously, it can also be handy when you don't want sound output at the moment, just want to read the value of a Feature. Let's check \"Probe follows idle mouse\" on the UI, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.probe_follows_idle_mouse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you hover your mouse over the app canvas, you can see that the Probe constantly follows it, and you can see indicators of the Probe's horizontal and vertical coordinates labelled \"Probe X\" and \"Probe Y\", respectively, and controlled by the `probe_x` and `probe_y` properties (as shown above).\n",
    "\n",
    "Let's move on to the Features pane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Features pane is where all your `Feature` objects (that you attached to the app!) will show up. (As of now, in pixasonics, the term \"feature\" always refers to a _visual_ feature, as currently audio feature extraction and mapping to auditory features is not supported.) \n",
    "\n",
    "Right now you should see a card labelled \"MeanRed\" there, our Feature that reports the mean red pixel value of the Probe's slice of the image. Since we just checked the option for the Probe to follow our idle (hovering) mouse, let's move it around a bit (without making sound), and look at the values we get on the card.\n",
    "\n",
    "To avoid scrolling back-and-forth too much, let's have another copy of our app here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every feature gets the slice of the image data under the Probe and reduces it into one or more values. Every time you move the probe with your mouse, all the features within the `App` are sent a new Probe matrix, and they update their feature buffers (the container where they store their most-recent values). This example feature that we named \"MeanRed\" measures the mean value of the Probe matrix in the red color channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the card you can see that, next to the name, there is a small identifier, which is uniquely generated for every different object. It is not very important now, but it might help you later identifying various objects in your pipeline.\n",
    "Underneath this header you see the \"Number of Features:\" box that shows you how many values does this `Feature` report. Now it's just 1, since we are only filtering for the first (red) channel. Before we discuss what's behind this in more detail, let's just move on and look at the rest of the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the card is occupied by 3 number fields, showing the minimum, maximum and current (or most-recent) value of the feature. When a `Feature` is attached to the `App`, it actually gets to look at the whole image _once_, so it can calculate things, like global minimum and maximum values. You probably noticed that as you move the Probe around, the Min and Max fields don't change. That is because it calculated the global minimum and maximum value in the entire image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resetting to running min and max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of the range of values will come especially handy when setting up mappings. In many cases it is helpful to get global min/max values, but sometimes we might need running min/max values of the Probe matrix only. If you hover your mouse over the Reset button in the bottom left corner of the Feature card, you will see the tooltip appear: _\"Reset to the running min and max\"_. You can click on that or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red.reset_minmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that at the time of resetting you got the same value as min and max. But when you move the Probe around, you see that the values start to move away from each other, as the running min and max values update. This behavior can be useful when you want to create a dynamic mapping and compare two visual objects relative to each other. It can also be convenient to compare parts of the image only visually, without sonification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach to and detach from the App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what about the other button that says \"Detach\"? Click on it, or evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach_feature(mean_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, our Feature is gone! Don't worry, the `mean_red` variable still holds our `Feature` object, it's just no longer included in the `App`'s pipeline. That is, whenever the Probe moves, this detached `Feature` won't get the updated Probe matrix anymore, and its values (its feature buffer) will remain the same. Just to prove that it's still there, here is how you can pull up its UI separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_red.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you go, our MeanRed feature is still functional. It is just outside the `App`'s pipeline. But we can attach it back by saying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach_feature(mean_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you go to the `App` UI again, you can see that now the card labelled MeanRed re-appeared in the Features pane. And by the way, our copy of the feature UI still works, it is in sync with the one in the `App` UI. Try to move the Probe and you'll se the numbers update in both places.\n",
    "\n",
    "This mechanism of \"attaching\" and \"detaching\" objects to and from the main app is a pattern that is the same for `Feature`s, Synths, and `Mapper`s. When you want to include something in the `App`, which is to say, you want to include it in the pipeline of Probe --> Features --> Mappers --> Synths, you _attach_ it to the App. When you don't need it anymore (or evne just for a while), you can _detach_ it from the App. The object will still \"live\", but it won't get updates from the `App` anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another `Feature` to see the mean pixel values across all three color channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pix = MeanChannelValue(name=\"MeanPix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall how we created `mean_red`? It was like this:\n",
    "```python\n",
    "# create a Feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only difference is that this time we did not specify `filter_channels` (so it defaults to `None`). By default all `Feature` objects process the entire Probe matrix which is actually a 4-dimensional matrix in the shape of:\n",
    "\n",
    "__(Height, Width, Channels, Layers)__\n",
    "\n",
    "_In fact, our entire image data is kept in that shape._ So when we specified `filter_channels=0`, we selected the 0th (red) channel in the 2nd, Channel dimension. Don't worry if this is a bit over your head now, the TLDR is that `Feature` objects can actually filter the Probe matrix in any way you need.\n",
    "\n",
    "Now let's attach our new `Feature` to our `App`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach_feature(mean_pix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the Features pane on our app UI has two cards now: MeanRed and MeanPix. We can also query the currently attached `Feature`s like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have probably noticed, MeanPix reports 3 values, which are the mean pixel value across the 3 channels of the Probe matrix. We'll get into more details about `Feature`s in a bit, but for now let's move on to Synths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synths are the tools to make sound with in pixasonics. As mentioned at the top, they are all abstractions (technically, `Patch`es) based on the awesome [signalflow library](https://signalflow.dev/).\n",
    "\n",
    "First let's have a look at a simple Theremin synth, outside the `App`. Here is how you make one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_theremin = Theremin(name=\"SimpleTheremin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull up its UI like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_theremin.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see we got a similar card here: a header with the name of the synth and its unique ID, a main section with all its parameters and corresponding sliders, as well as Reset and Detach buttons at the bottom of the card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is nothing there though that turns the synth on or off. This is normally done by our `App` as a result of our interaction with the Probe. Under the hood, the `App` creates (or resets) the global audio graph in signalflow, which is the `AudioGraph` object. While this is not the intended use case, we can use `App.graph` to reference this global audio graph and play our synth patch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = app.graph\n",
    "graph.play(simple_theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Parameters and Resetting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should hear a static sine tone. Try moving the sliders and listen to how the sound changes to get an understanding of the parameters. When you are done, you can hit the Reset button to reset all parameters to their defaults. These defaults are not set in stone, they are decided when we create the synth object. If we spell it out, we actually said this:\n",
    "```python\n",
    "simple_theremin = Theremin(frequency=440, amplitude=0.5, panning=0, name=\"SimpleTheremin\")\n",
    "```\n",
    "\n",
    "Now let's stop playing the synth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.stop(simple_theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping to parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pixasonics we currently focus on Parameter Mapping Sonification. This is probably the most common sonification technique, where data features get mapped to input parameters of a synthesizer. While this might have some perceptual issues when mappings get complex, there is a reason why this technique is so popular: it is very simple to set up and it is very explainable.\n",
    "In the future we do plan to support other types of mappings as well though, but for now, let's focus on how we can express quantities in the image (such as the level of red channel values) as changes in the parameters of synths, such as the frequency.\n",
    "\n",
    "Let's make a new instance of our app UI again for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attach and detach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the Synths pane. Just like the Features pane, this is where all the attached Synths will have their UIs. Similarly to `Feature`s, you can detach Synths by clicking on the Detach button on their card, or by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.detach_synth(theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means our Theremin is not included in the `App`'s audio graph anymore, so when we activate the probe, there is no sound (since there is no Synth currently attached to the `App`).\n",
    "\n",
    "We can attach the Theremin back by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.attach_synth(theremin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving sliders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to activate the Probe, listen to the sound and watch the frequency slider of the Theremin. (Remember, we are still in \"Toggle\" interaction mode, so you have to double-click to activate the Probe.)\n",
    "\n",
    "The slider of the Theremin seems to move on its own as we move the Probe. Have you observed any relationship between the image data and the frequency of the Theremin? If you need a hint, open the Mappers pane on the app UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mappers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our very first setup of this scene, we created and attached this `Mapper` like so:\n",
    "```python\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(mean_red, theremin[\"frequency\"], exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach_mapper(red2freq) # this adds it to the pipeline, so that it is updated every frame (the Probe moves)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Mapper`s take two positional arguments as the `Feature` object we want to map _from_ and the Synth parameter we want to map _to_. The notation `theremin[\"frequency\"]` will tell the `Mapper` to map to the frequency parameter of the theremin synth object. The actual object that is returned is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theremin[\"frequency\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranges at the input and output sides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of info in here that we don't need to worry about right now. The most important for us now are the two top ones, the 'min' and 'max'.\n",
    "\n",
    "What a `Mapper` does, is mapping a value (or an array of values) from a given input range to a given output range. It can do this either linearly (where `exponent=1`) or exponentially (like `exponent=2` for a cubic curve).\n",
    "\n",
    "But we haven't specified any input or output range when we created our `red2freq` mapper object. How can it still know what red values to map to what frequency value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is knowing that both `Feature` objects report their global (or running) min and max values (remember?!) and that all Synth parameter dictionaries will contain curated (sensible) min and max values. If no input and output ranges are specified upon creation, the `Mapper` object will just look up the corresponding min and max values on both the feature-side and the synth-side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific ranges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having automatic ranges that update live with the running min/max of a `Feature` object can be handy for experimentation, but sometimes we do want to specify something more concrete. Before we get there, it is perhaps helpful to look at the spelled-out verison of how we created our `red2freq` object:\n",
    "```python\n",
    "red2freq = Mapper(\n",
    "    obj_in=mean_red, \n",
    "    obj_out=theremin[\"frequency\"],\n",
    "    in_low=None,\n",
    "    in_high=None,\n",
    "    out_low=None,\n",
    "    out_high=None,\n",
    "    exponent=2, \n",
    "    clamp=True,\n",
    "    name=\"Red2Freq\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify an input range (on the `Feature` side) with the `in_low` and `out_low` arguments, while the output range (on the Synth parameter side) with the `out_low` and `out_high` arguments. They all default to `None`, which will mean that the `Mapper` will look up the current min/max reported by the `Feature` and the default min and max values of the Synth param (that were decided when we created the Synth).\n",
    "\n",
    "We can also retrieve these values like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low, red2freq.in_high, red2freq.out_low, red2freq.out_high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even though our MeanRed `Feature` reports singular scalar values, the data is in a Numpy array of shape `(1, 1)`. This technical detail will be more important when we want to map a vector of features to a multichannel synth, but let's ignore that case for now\n",
    "\n",
    "What is important is that we can now override the ranges like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low = 150\n",
    "red2freq.in_high = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's listen to what has changed in the mapping.\n",
    "\n",
    "You probably notice that for most cells the sound remains at its minimum, while for the really bright ones it suddenly chirps up to a really high whistle. Let's go back to using the automatic ranges by setting the `in_low` and `in_high` props to `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.in_low = None\n",
    "red2freq.in_high = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a listen!\n",
    "\n",
    "We are back to the kind of mapping we started out with. It is striking how little change resulted in such a drastic change in the sound pattern, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by the way, in the previous setting, why did all the cells that were not so bright get the same static low hum? That is because by default we clamp (limit) the mapped values to `out_low` and `out_high`. You probably want clamping to be on for most of the time, but if you know what you are doing, consider turning it off (by writing `clamp=False`) and then any input value that is below or above `in_low` or `in_high` will get proportionally mapped to below `out_low` or `out_high`. But be careful, this can quickly explode values, especially when you use exponential scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last important parameter of a `Mapper` is the exponent to use for the mapping. Think about it as the mapping \"curve\". For a linar mapping, where `exponent=1` we will get a curve like this:\n",
    "\n",
    "![linear curve](figures/exponent_1.png)\n",
    "\n",
    "But we might want an exponential or a logarithmic curve for our mapping. Here are some examples:\n",
    "\n",
    "`exponent=2`\n",
    "\n",
    "![exponent equals 2](figures/exponent_2.png)\n",
    "\n",
    "`exponent=4`\n",
    "\n",
    "![exponent equals 4](figures/exponent_4.png)\n",
    "\n",
    "`exponent=0.5`\n",
    "\n",
    "![exponent equals 0.5](figures/exponent_0.5.png)\n",
    "\n",
    "`exponent=0.25`\n",
    "\n",
    "![exponent equals 0.25](figures/exponent_0.25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we were mapping to frequencies (measured in Hz) we actually get preceptually more \"linear\" feel in the mapping if the curve is exponential, that is why we set `exponent=2` in our `red2freq` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red2freq.exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import signalflow as sf\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "from pixasonics.core import App, Mapper\n",
    "from pixasonics.features import *\n",
    "from pixasonics.synths import Theremin, Oscillator, FilteredNoise, SimpleFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create app\n",
    "app = App()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading HDR images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load HDR images\n",
    "img_path = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w1.TIF\"\n",
    "app.load_image_file(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image as a numpy array\n",
    "img_path = \"images/cellular_dataset/merged_8bit/Timepoint_005_220518-ST_C03_s1.jpg\"\n",
    "img = Image.open(img_path)\n",
    "img = np.array(img)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two HDR images and load as numpy array\n",
    "img_path = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w2.TIF\"\n",
    "img_path2 = \"images/cellular_dataset/single_channel_16bit/Timepoint_005_220518-ST_C03_s1_w1.TIF\"\n",
    "img = Image.open(img_path)\n",
    "img2 = Image.open(img_path2)\n",
    "img = np.array(img)\n",
    "img2 = np.array(img2)\n",
    "img = np.stack([img, img2], axis=-1)\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all red-ch images into arrays and concatenate them in the channel dimension\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "img_files = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs = []\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the channel dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all red-ch images into arrays and concatenate them in the layer dimension\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "img_files = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs = []\n",
    "for img_file in img_files:\n",
    "    img_path = os.path.join(img_folder, img_file)\n",
    "    img = Image.open(img_path)\n",
    "    img = np.array(img)[..., None] # add a new dimension for channels\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine red and green channels and all layers\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs = []\n",
    "for img_red, img_green in zip(imgs_red, imgs_green):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img = np.stack([img_red, img_green], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use all images in the folder\n",
    "img_folder = \"images/cellular_dataset/single_channel_16bit/\"\n",
    "img_files = os.listdir(img_folder)\n",
    "imgs_red = [f for f in img_files if f.endswith(\"w2.TIF\")] # only red channel images\n",
    "imgs_green = [f for f in img_files if f.endswith(\"w1.TIF\")] # only green channel images\n",
    "imgs_blue = [f for f in img_files if f.endswith(\"w3.TIF\")] # only blue channel images\n",
    "imgs = []\n",
    "for img_red, img_green, img_blue in zip(imgs_red, imgs_green, imgs_blue):\n",
    "    img_path_red = os.path.join(img_folder, img_red)\n",
    "    img_path_green = os.path.join(img_folder, img_green)\n",
    "    img_path_blue = os.path.join(img_folder, img_blue)\n",
    "    img_red = Image.open(img_path_red)\n",
    "    img_green = Image.open(img_path_green)\n",
    "    img_blue = Image.open(img_path_blue)\n",
    "    img_red = np.array(img_red)\n",
    "    img_green = np.array(img_green)\n",
    "    img_blue = np.array(img_blue)\n",
    "    img = np.stack([img_red, img_green, img_blue], axis=-1) # now the last dimension is the channel dimension\n",
    "    imgs.append(img)\n",
    "img = np.stack(imgs, axis=-1) # now the last dimension is the layer dimension\n",
    "print(img.shape)\n",
    "app.load_image_data(img) # load as numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-real-time rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example minimal setup\n",
    "\n",
    "# create a feature that will report the mean value of the red channel\n",
    "mean_red = MeanChannelValue(filter_channels=0, name=\"MeanRed\")\n",
    "# attach the feature to the app\n",
    "app.attach_feature(mean_red)\n",
    "\n",
    "# create a Theremin, a simple sine wave synth that we will use to sonify the mean pixel value\n",
    "theremin = Theremin()\n",
    "# attach the Theremin to the app\n",
    "app.attach_synth(theremin)\n",
    "\n",
    "# create a Mapper that will map the mean red pixel value (within the Probe) to the frequency of the Theremin\n",
    "red2freq = Mapper(\n",
    "    mean_red, \n",
    "    theremin[\"frequency\"], \n",
    "    in_low=200,\n",
    "    in_high=500,\n",
    "    exponent=2, name=\"Red2Freq\") # cubic mapping curve for a more \"linear\" feel of frequency changes\n",
    "# attach the Mapper to the app\n",
    "app.attach_mapper(red2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: horizontal scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"horizontal_scan.wav\"\n",
    "\n",
    "app.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: long horizontal scan, so we reduce app fps to optimize rendering\n",
    "app.fps = 10\n",
    "duration = 60\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 1,\n",
    "        \"probe_height\": 500,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_x\": 499\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"long_horizontal_scan.wav\"\n",
    "\n",
    "app.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename))\n",
    "app.fps = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example: vertical scan\n",
    "duration = 5\n",
    "my_timeline = [\n",
    "    (0, {\n",
    "        \"probe_width\": 500,\n",
    "        \"probe_height\": 1,\n",
    "        \"probe_x\": 0,\n",
    "        \"probe_y\": 0\n",
    "    }),\n",
    "    (duration, {\n",
    "        \"probe_y\": 499\n",
    "    })\n",
    "]\n",
    "\n",
    "target_filename = \"vertical_scan.wav\"\n",
    "\n",
    "app.render_timeline_to_file(my_timeline, target_filename)\n",
    "\n",
    "display(Audio(target_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Feature base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults\n",
    "base_feature = Feature(\n",
    "    filter_rows=None,\n",
    "    filter_columns=None,\n",
    "    filter_channels=None,\n",
    "    filter_layers=None,\n",
    "    target_dim=2,\n",
    "    reduce_method=\"mean\",\n",
    "    name=\"Feature\"\n",
    ")\n",
    "app.attach_feature(base_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean pixel value\n",
    "mean_pix = Feature(name=\"Mean\") # defaults to mean pixel value, channel dimension is kept\n",
    "app.attach_feature(mean_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median pixel value\n",
    "median_pix = Feature(name=\"Median\", reduce_method=\"median\")\n",
    "app.attach_feature(median_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean row value (will change the number of features depending of the Probe height)\n",
    "mean_row = Feature(\n",
    "    name=\"Mean Row\",\n",
    "    target_dim=0, # mean value of each row (for horizontal scanning)\n",
    ")\n",
    "app.attach_feature(mean_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Median, filter for the red (first) channel only\n",
    "median_red = Feature(\n",
    "    name=\"Median Red\",\n",
    "    target_dim=2, # channel dim\n",
    "    filter_channels=0, # only use the first channel (red)\n",
    "    reduce_method=\"median\"\n",
    ")\n",
    "app.attach_feature(median_red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multichannel Synths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 16-voice oscillator bank of sawtooth waves, spread across the stereo field\n",
    "num_voices = 16\n",
    "freqs = [440 for i in range(num_voices)]\n",
    "osc_bank = Oscillator(frequency=freqs, panning=[-1, 1], waveform=\"saw\", name=\"OSCBank\") # test multichannel\n",
    "app.attach_synth(osc_bank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping where each row value will be mapped to a different channel of the oscillator bank\n",
    "row2freq = Mapper(mean_row, osc_bank[\"frequency\"], exponent=2, out_high=1000, name=\"Row2Freq\")\n",
    "app.attach_mapper(row2freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.ui"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pixasonics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
